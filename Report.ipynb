{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "The goal of this project is to solve the [Collaboration and Competition](https://github.com/udacity/deep-reinforcement-learning/tree/master/p3_collab-compet) challenge from the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Required Libraries\n",
    "\n",
    "The following libraries and dependencies are used:\n",
    "\n",
    "1. [Python 3.6](https://www.python.org/downloads/)\n",
    "2. [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)\n",
    "3. [NumPy](http://www.numpy.org/)\n",
    "4. [Pytorch 1.3](https://pytorch.org/)\n",
    "\n",
    "And the following files has been defined:\n",
    "\n",
    "1. agent.py : Contains the implementation of Random, DDPG agents and MADDPG.\n",
    "2. model.py : Contains the Actor and Critic models used by the DDPG agents.\n",
    "3. noise.py : Contains the implementation of the Ornstein–Uhlenbeck noise.\n",
    "3. coach.py : Contains a function to run the environment with a specified agent and define the structure to learn from the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "from agent import RandomAgent, DDPGAgent, MADDPG\n",
    "from coach import Coach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.The Environment\n",
    "\n",
    "The compiled environment can be downloaded from the following links:\n",
    "\n",
    "- [Linux](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip)\n",
    "- [Mac OSX](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip)\n",
    "- [Windows (32-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip)\n",
    "- [Windows (64-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip)\n",
    "\n",
    "Once downloaded, please update the environment location below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_location = \"env/Tennis.app\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=environment_location)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation and Action Space\n",
    "\n",
    "The environment consists of two agents (represented as rackets) that can be controlled to play a tennis-like game. Each agent receives a reward of +0.1 if it hits the ball while receives a penality of -0.01 when the ball hits the ground on its territory. To maximise the environment reward, both agent should try to keep the ball in play as long as possible.\n",
    "\n",
    "Each agent observation is defined by 8 variables corresponding to the position and velocity of the ball and its racket. The ensemble of the two observations will be called \"state\" here since it has all the information required to re-create the current state.\n",
    "\n",
    "Actions are continious and are defined by two variables, one to move toward/away from the net and one to jump.\n",
    "\n",
    "The episode score is defined as the maximum score of the 2 agents. The environment is considered solved if this score reaches +0.5 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Each agent makes an observation of length: 24\n",
      "And can make an action of length: 2\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "n_agents = len(env_info.agents)\n",
    "action_space = brain.vector_action_space_size\n",
    "state = env_info.vector_observations\n",
    "\n",
    "observation_space = state.shape[1]\n",
    "\n",
    "print('Number of agents:', n_agents)\n",
    "print('Each agent makes an observation of length: {}'.format(observation_space))\n",
    "print('And can make an action of length: {}'.format(action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Settings and Parameters\n",
    "\n",
    "The following settings and parameters have been used to train the MADDPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128                   # Batch size for training neural network\n",
    "gamma = 0.995                      # Discount factor for future rewards\n",
    "replay_buffer_size = int(1e6)      # Memory buffer size\n",
    "actor_lr = 1e-4                    # Learning rate of the local actor \n",
    "critic_lr = 1e-3                   # Learning rate of the local critic \n",
    "n_updates = 5                      # Number of learning updates\n",
    "tau = 5e-3                         # Hyperparameter for the soft-updates\n",
    "\n",
    "action_range = [-1.,1.]\n",
    "\n",
    "eps_decay = 0.999                  # Decay of the Ornstein–Uhlenbeck Noise\n",
    "min_eps = 0.001                    # Minimum noise multiplier after exploration\n",
    "\n",
    "n_episodes = 2000                  # Number of episodes on which to train\n",
    "max_steps = int(1e9)               # Maximum number of frames per episode\n",
    "log_interval = 100                 # Number of episodes before a fixed log of results\n",
    "save_interval = 100                # Number of episodes before saving model\n",
    "\n",
    "save_directory = 'Checkpoint\\\\'    # Directory where models will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Defining the Coach\n",
    "The Coach will be responsible of running the agent and supervising its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coach = Coach(env=env,\n",
    "              brain_name=brain_name,\n",
    "              save_directory=save_directory\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run a random agent in the environment in order to better understand the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = RandomAgent(n_agents=n_agents,\n",
    "                           action_space=action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coach.watch(agent=random_agent, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create and train Multi Agent DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_agent = MADDPG(n_agents=n_agents,\n",
    "                 observation_space=observation_space, \n",
    "                 action_space=action_space,\n",
    "                 action_range=action_range,\n",
    "                 replay_buffer_size=replay_buffer_size, \n",
    "                 batch_size=batch_size, \n",
    "                 gamma=gamma, \n",
    "                 tau=tau,\n",
    "                 actor_lr=actor_lr, \n",
    "                 critic_lr=critic_lr,\n",
    "                 eps_decay=eps_decay,\n",
    "                 min_eps=min_eps,\n",
    "                 n_updates=n_updates,\n",
    "                 seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  100/2000 | Cum.Avg.Score: 0.003 | Epis.Score: 0.000 | Elaps.Time: 0h 10m 46s\n",
      "Episode:  200/2000 | Cum.Avg.Score: 0.006 | Epis.Score: 0.000 | Elaps.Time: 0h 23m 08s\n",
      "Episode:  300/2000 | Cum.Avg.Score: 0.053 | Epis.Score: 0.000 | Elaps.Time: 0h 44m 32s\n",
      "Episode:  400/2000 | Cum.Avg.Score: 0.053 | Epis.Score: 0.100 | Elaps.Time: 1h 06m 54s\n",
      "Episode:  500/2000 | Cum.Avg.Score: 0.072 | Epis.Score: 0.200 | Elaps.Time: 1h 33m 25s\n",
      "Episode:  600/2000 | Cum.Avg.Score: 0.088 | Epis.Score: 0.100 | Elaps.Time: 2h 04m 02s\n",
      "Episode:  700/2000 | Cum.Avg.Score: 0.124 | Epis.Score: 0.100 | Elaps.Time: 2h 45m 16s\n",
      "Episode:  800/2000 | Cum.Avg.Score: 0.194 | Epis.Score: 0.100 | Elaps.Time: 3h 48m 10s\n",
      "Episode:  900/2000 | Cum.Avg.Score: 0.250 | Epis.Score: 0.100 | Elaps.Time: 5h 09m 13s\n",
      "Episode:  956/2000 | Cum.Avg.Score: 0.386 | Epis.Score: 2.500 | Elaps.Time: 6h 38m 59s"
     ]
    }
   ],
   "source": [
    "scores, cum_scores = coach.train(agent=m_agent, \n",
    "                                 n_episodes=n_episodes, \n",
    "                                 max_steps=max_steps,\n",
    "                                 log_interval=log_interval,\n",
    "                                 save_interval=save_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(rewards)), np.asarray(rewards), c='lightsteelblue', linestyle=':', label='Episode Score')\n",
    "ax.plot(np.arange(len(rewards)), np.asarray(cum_rewards), c='royalblue', label='Average 100 last scores')\n",
    "ax.set(xlabel='Episodes', ylabel='Score')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Watch the trained MADDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coach.watch(m_agent,n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ideas for future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We could do an hyperparameters tuning to train the agents in less epochs.\n",
    "<br><br>\n",
    "2. Some of the code for the learning part could be optimized to run faster.\n",
    "<br><br>\n",
    "3. Since each agent is the reflexion of the other, we could increase the replay memory by a factor of 2.\n",
    "<br><br>\n",
    "4. Since the reward is really sparse at the beginning, we could try different noise functions that would provide a better result at the begginning, but this might be environment specific...\n",
    "<br><br>\n",
    "5. The reward function could maybe be adjusted to give a bonus for how close the racket is from the ball. The model could then start to learn to go near the ball earlier without trial and error as the current implementation is doing.\n",
    "<br><br>\n",
    "6. We could adapt the code to train on the [soccer environment](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
